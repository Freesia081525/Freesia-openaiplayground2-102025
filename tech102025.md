¬†Hi please improve the code based on the sample code and adding additonal feature that user can click paste request/prompt click a button then system will use the request/prompt to execute following openai api:from openai import OpenAI
client = OpenAI()

response = client.responses.create(
  prompt={
    "id": "pmpt_68f61aede8108194820e1e47629c13eb053a19ea06b40b1f",
    "version": "1"
  }
) . Ending with follow up questions. Think deep and iterate to get excellent results. Fix potential bugs to be deployed on hugging face space using streamlit, agents.yaml before give results. Sample code:Below is a drop-in upgraded Streamlit app with a ‚Äúwow‚Äù UI, status indicators, and an interactive dashboard. It also adds the new features you requested:
- [1] File-to-Markdown pipeline: upload/paste TXT/MD/PDF with optional OCR by page selection; markdown transformation with coral keyword highlighting; editable summary with 100-entity table, 30 Q&A, and 10 follow-ups; choose and edit an agent from agents.yaml, choose a provider/model, modify prompts/params, execute and edit the response.
- [2] Multi-dataset pipeline: upload/paste multiple datasets (TXT/CSV/JSON), auto assign IDs/names; render as markdown tables and editable tables; transform each into JSON and editable; comprehensive cross-dataset analysis with visualization suggestions, 100 entities, 30 Q&A, 10 follow-ups; agent selection/execution; interactive dashboard for insights.
- Multi-provider model routing: Gemini (gemini-2.5-flash, gemini-2.5-flash-lite), OpenAI (gpt-5-nano, gpt-4.1-nano), Grok (grok-3-mini, grok-4-fast-reasoning).
- Mind Graph upgraded with theme presets, node color control, search/filter, and dashboard metrics.

Replace your app code with this:

```python
# file: app.py
import os
import io
import re
import ast
import sys
import json
import uuid
import yaml
import base64
import random
import textwrap
import datetime as dt
from dataclasses import dataclass
from typing import List, Dict, Any, Tuple, Optional

import pandas as pd
import numpy as np
import streamlit as st
from streamlit_agraph import agraph, Node, Edge, Config

# Optional imports with graceful fallback
def try_import(module_name, alias=None):
    try:
        m = __import__(module_name) if alias is None else __import__(module_name, fromlist=[alias])
        return m if alias is None else getattr(m, alias)
    except Exception:
        return None

px = try_import("plotly.express")
go = try_import("plotly.graph_objects")

pdfplumber = try_import("pdfplumber")
pytesseract = try_import("pytesseract")
pdf2image = try_import("pdf2image")
PIL_Image = None
try:
    from PIL import Image as PIL_Image
except Exception:
    PIL_Image = None

# LLM SDKs (lazy)
google_genai = None
openai = None
xai_Client = None
xai_grok_user = None

# ----------------------------
# App configuration
# ----------------------------
st.set_page_config(page_title="Agentic Mind Studio", page_icon="üß†", layout="wide")

# ----------------------------
# Custom CSS (WOW UI)
# ----------------------------
PRIMARY = "#7C4DFF"
ACCENT = "#FF7F50"  # Coral
BG_GRADIENT = """
linear-gradient(135deg, rgba(16,20,33,1) 0%, rgba(18,18,18,1) 35%, rgba(38,35,63,1) 100%)
"""
CARD_BG = "rgba(255,255,255,0.06)"
BORDER = "rgba(255,255,255,0.15)"

st.markdown(f"""
<style>
    .stApp {{
        background: {BG_GRADIENT};
        color: #EDEDED;
        font-family: 'Inter', system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, Cantarell, 'Helvetica Neue', Arial, 'Noto Sans', 'Apple Color Emoji', 'Segoe UI Emoji';
    }}
    .wow-header {{
        padding: 16px 24px;
        border-radius: 16px;
        background: {CARD_BG};
        border: 1px solid {BORDER};
        backdrop-filter: blur(10px);
        margin-bottom: 12px;
    }}
    .wow-badge {{
        display: inline-block;
        padding: 2px 10px;
        border-radius: 999px;
        font-size: 12px;
        color: #fff;
        background: {PRIMARY};
        margin-right: 8px;
    }}
    .card {{
        background: {CARD_BG};
        border: 1px solid {BORDER};
        border-radius: 14px;
        padding: 14px 16px;
        margin-bottom: 10px;
    }}
    .metric-card {{
        background: {CARD_BG};
        border: 1px solid {BORDER};
        border-radius: 14px;
        padding: 14px 16px;
        text-align: center;
    }}
    .metric-value {{
        font-size: 26px;
        font-weight: 700;
        color: #FFFFFF;
    }}
    .metric-label {{
        font-size: 12px;
        opacity: 0.8;
    }}
    .kpi-grid {{
        display: grid;
        grid-template-columns: repeat(4, 1fr);
        gap: 10px;
        margin-bottom: 8px;
    }}
    .small {{
        font-size: 12px;
        opacity: 0.85;
    }}
    .stTabs [data-baseweb="tab-list"] button {{
        background: {CARD_BG};
        border: 1px solid {BORDER};
        margin-right: 6px;
        border-radius: 12px;
    }}
    .stTabs [data-baseweb="tab"] {{
        color: #fff;
    }}
    .stTextArea textarea, .stTextInput input {{
        background: rgba(255,255,255,0.05);
        border: 1px solid {BORDER};
        color: #fff;
    }}
    .stDataFrame, .stDataEditor {{
        filter: invert(0.0) hue-rotate(0deg);
    }}
    .st-emotion-cache-1r4qj8v, .st-emotion-cache-kgpedg {{
        background: transparent !important;
    }}
    .coral {{ color: {ACCENT}; font-weight: 600; }}
</style>
""", unsafe_allow_html=True)

# ----------------------------
# State
# ----------------------------
if "datasets" not in st.session_state:
    st.session_state.datasets = {}  # id -> {"name":..., "df":..., "json":...}
if "relationships" not in st.session_state:
    st.session_state.relationships = []
if "graph_theme" not in st.session_state:
    st.session_state.graph_theme = "Midnight"
if "agents_config" not in st.session_state:
    st.session_state.agents_config = None
if "logs" not in st.session_state:
    st.session_state.logs = []

def log_event(kind: str, message: str, extra: Optional[dict] = None):
    st.session_state.logs.append({
        "ts": dt.datetime.utcnow().isoformat() + "Z",
        "kind": kind,
        "message": message,
        "extra": extra or {}
    })

# ----------------------------
# Sidebar: Providers, Models, API Keys
# ----------------------------
st.sidebar.subheader("Model Providers")

provider = st.sidebar.selectbox("Provider", ["Gemini", "OpenAI", "Grok"], index=0)

if provider == "Gemini":
    gemini_key = st.sidebar.text_input("Gemini API Key", type="password")
    gemini_model = st.sidebar.selectbox("Gemini Model", ["gemini-2.5-flash", "gemini-2.5-flash-lite"])
    if gemini_key:
        try:
            google_genai = __import__("google.generativeai")
            google_genai.configure(api_key=gemini_key)
        except Exception as e:
            st.sidebar.error(f"Gemini SDK error: {e}")
elif provider == "OpenAI":
    openai_key = st.sidebar.text_input("OpenAI API Key", type="password")
    openai_model = st.sidebar.selectbox("OpenAI Model", ["gpt-5-nano", "gpt-4.1-nano"])
    if openai_key:
        try:
            openai = __import__("openai")
            openai.api_key = openai_key
        except Exception as e:
            st.sidebar.error(f"OpenAI SDK error: {e}")
else:
    grok_key = st.sidebar.text_input("Grok API Key", type="password")
    grok_model = st.sidebar.selectbox("Grok Model", ["grok-3-mini", "grok-4-fast-reasoning"])
    if grok_key:
        try:
            xai_Client = __import__("xai_sdk").Client
            xai_grok_user = __import__("xai_sdk.chat", fromlist=["user"]).user
            # We instantiate per-call to set timeout
        except Exception as e:
            st.sidebar.error(f"Grok SDK error: {e}")

# Theme for graph
st.sidebar.subheader("Theme & Colors")
themes = {
    "Midnight": {"bg": "#0F1220", "edge": "#7C4DFF", "font": "#EDEDED"},
    "Sky Blue": {"bg": "#E6F7FF", "edge": "#3399FF", "font": "#003366"},
    "Deep Sea": {"bg": "#001F3F", "edge": "#0074D9", "font": "#7FDBFF"},
    "Fendi Luxury": {"bg": "#2b2520", "edge": "#6D4C41", "font": "#FFF0D8"},
}
theme_choice = st.sidebar.selectbox("Graph Theme", list(themes.keys()), index=list(themes.keys()).index(st.session_state.graph_theme))
st.session_state.graph_theme = theme_choice
graph_theme = themes[theme_choice]
node_color = st.sidebar.color_picker("Node color", ACCENT)

# ----------------------------
# Utility functions
# ----------------------------
def parse_csv_or_lines(text: str) -> pd.DataFrame:
    try:
        return pd.read_csv(pd.io.common.StringIO(text))
    except Exception:
        lines = [l.strip() for l in text.splitlines() if l.strip()]
        return pd.DataFrame({"record": lines})

def read_any_dataset(file, pasted_text: Optional[str]) -> Tuple[Optional[pd.DataFrame], Optional[list]]:
    if file is not None:
        name = file.name.lower()
        if name.endswith(".csv"):
            df = pd.read_csv(file)
        elif name.endswith(".json"):
            data = json.load(file)
            df = pd.DataFrame(data if isinstance(data, list) else [data])
        elif name.endswith(".txt") or name.endswith(".md") or name.endswith(".markdown"):
            content = file.read().decode("utf-8", errors="ignore")
            df = parse_csv_or_lines(content)
        else:
            st.error("Unsupported file format.")
            return None, None
    elif pasted_text:
        try:
            data = json.loads(pasted_text)
            df = pd.DataFrame(data if isinstance(data, list) else [data])
        except Exception:
            try:
                df = pd.read_csv(pd.io.common.StringIO(pasted_text))
            except Exception:
                df = parse_csv_or_lines(pasted_text)
    else:
        return None, None
    return df, df.to_dict(orient="records")

def parse_pdf_pages(pages_spec: str) -> List[int]:
    # example: "1-3,5,9-10"
    pages = set()
    for part in re.split(r"[,\s]+", pages_spec.strip()):
        if not part:
            continue
        if "-" in part:
            a, b = part.split("-", 1)
            try:
                a, b = int(a), int(b)
                for p in range(min(a, b), max(a, b) + 1):
                    pages.add(p)
            except Exception:
                pass
        else:
            try:
                pages.add(int(part))
            except Exception:
                pass
    return sorted(list(pages))

def extract_pdf_text(file, use_ocr: bool, pages: List[int]) -> str:
    buf = file.read()
    bio = io.BytesIO(buf)
    if not use_ocr and pdfplumber is not None:
        with pdfplumber.open(bio) as pdf:
            selected_pages = pages if pages else list(range(1, len(pdf.pages) + 1))
            texts = []
            for p in selected_pages:
                if 1 <= p <= len(pdf.pages):
                    texts.append(pdf.pages[p-1].extract_text() or "")
            return "\n".join(texts).strip()
    # OCR path
    if pdf2image is None or pytesseract is None:
        return "ERROR: OCR libraries not available. Please install pdf2image and pytesseract, and ensure tesseract is installed."
    try:
        images = pdf2image.convert_from_bytes(buf)
        selected_pages = pages if pages else list(range(1, len(images) + 1))
        texts = []
        for p in selected_pages:
            if 1 <= p <= len(images):
                img = images[p-1]
                text = pytesseract.image_to_string(img)
                texts.append(text)
        return "\n".join(texts).strip()
    except Exception as e:
        return f"ERROR during OCR: {e}"

def coral_highlight_markdown(text: str, keywords: List[str], color_hex: str = ACCENT) -> str:
    escaped = [re.escape(k) for k in keywords if k.strip()]
    if not escaped:
        return text
    pattern = r"(" + r"|".join(escaped) + r")"
    def repl(m):
        return f'<span style="color:{color_hex}; font-weight:600">{m.group(1)}</span>'
    try:
        return re.sub(pattern, repl, text, flags=re.IGNORECASE)
    except Exception:
        return text

def naive_keywords(text: str, topn: int = 20) -> List[str]:
    words = re.findall(r"[A-Za-z][A-Za-z0-9_-]{3,}", text.lower())
    stop = set("""
        the of a an and or to is are am be in on for with by as at that this these those it from
        was were will would can could should may might have has had into about across between
        you your we they them he she his her their our not no yes if else when where which who
        """.split())
    freq = {}
    for w in words:
        if w in stop:
            continue
        freq[w] = freq.get(w, 0) + 1
    return [w for w, c in sorted(freq.items(), key=lambda x: -x[1])[:topn]]

def heuristic_entities(text: str, n: int = 100):
    kws = naive_keywords(text, topn=n)
    return [{"entity": k, "type": "keyword", "salience": round(random.uniform(0.2, 0.99), 2)} for k in kws]

def call_llm_text(provider: str, model: str, prompt: str, temperature: float = 0.2, max_tokens: int = 3000) -> str:
    if provider == "Gemini" and google_genai:
        try:
            gm = google_genai.GenerativeModel(model)
            resp = gm.generate_content(prompt)
            return getattr(resp, "text", "").strip()
        except Exception as e:
            return f"LLM error (Gemini): {e}"
    elif provider == "OpenAI" and openai:
        try:
            # Fallback to chat completions API
            kw = {"model": model, "messages": [{"role": "user", "content": prompt}], "temperature": temperature}
            # Different OpenAI Python versions vary:
            if hasattr(openai, "chat") and hasattr(openai.chat, "completions"):
                out = openai.chat.completions.create(**kw)
                return out.choices[0].message["content"].strip()
            else:
                out = openai.ChatCompletion.create(**kw)
                return out["choices"][0]["message"]["content"].strip()
        except Exception as e:
            return f"LLM error (OpenAI): {e}"
    elif provider == "Grok" and xai_Client and xai_grok_user:
        try:
            client = xai_Client(api_key=grok_key, timeout=3600)
            chat = client.chat.create(model=model)
            chat.append(xai_grok_user(prompt))
            resp = chat.sample()
            return resp.content.strip()
        except Exception as e:
            return f"LLM error (Grok): {e}"
    else:
        return "LLM not configured. Provide API key."

def call_llm_structured(provider: str, model: str, instruction: str, text_block: str) -> Dict[str, Any]:
    # We ask the model to return JSON with specific keys; we'll parse if possible.
    prompt = f"""
You are a precise extractor and summarizer. Using the INPUT below, produce a JSON object with:
- "entities": array of 100 objects with keys ["entity","type","salience"]
- "qa": array of 30 objects with keys ["question","answer"]
- "followups": array of 10 strings
- "summary_markdown": a well-structured Markdown summary with sections and tables where helpful.

Return ONLY valid JSON, no backticks, no commentary.

INPUT:
{text_block}
"""
    out = call_llm_text(provider, model, prompt, temperature=0.2, max_tokens=6000)
    try:
        # remove potential code fences
        if out.strip().startswith("```"):
            out = "\n".join(out.strip().splitlines()[1:-1])
        data = json.loads(out)
        # basic validation
        data.setdefault("entities", [])
        data.setdefault("qa", [])
        data.setdefault("followups", [])
        data.setdefault("summary_markdown", "")
        return data
    except Exception:
        # fallback heuristic if model fails or not configured
        ents = heuristic_entities(text_block, n=100)
        qas = [{"question": f"Q{i+1}: What is '{e['entity']}'?", "answer": "Context-dependent."} for i, e in enumerate(ents[:30])]
        fus = [f"Follow-up {i+1}: Explore relationships of key terms." for i in range(10)]
        summary = "## Summary\n\n- Heuristic summary based on keyword frequency.\n"
        return {"entities": ents, "qa": qas, "followups": fus, "summary_markdown": summary}

def heuristic_relationships(records: List[dict]):
    pairs = set()
    for r in records:
        project = r.get("project") or r.get("title")
        team = r.get("team") or r.get("department")
        author = r.get("author")
        log = r.get("log_entry") or r.get("log")
        if project and team: pairs.add((str(project), str(team)))
        if project and author: pairs.add((str(project), f"Author: {author}"))
        if project and log: pairs.add((str(project), str(log)[:50]))
    return list(pairs)

def build_graph_elements(relationships, node_color_hex, theme: dict):
    nodes_set = set()
    for s, t in relationships:
        nodes_set.add(s)
        nodes_set.add(t)
    nodes = [Node(id=i, label=name, size=24, color=node_color_hex) for i, name in enumerate(sorted(nodes_set))]
    idx = {n.label: n.id for n in nodes}
    edges = [Edge(source=idx.get(s), target=idx.get(t), color=theme["edge"]) for s, t in relationships if s in idx and t in idx]
    return nodes, edges, idx

def records_by_node(node_label: str, records: List[dict]):
    results = []
    for r in records:
        if node_label.lower() in json.dumps(r, ensure_ascii=False).lower():
            results.append(r)
    return results

def df_to_markdown(df: pd.DataFrame, max_rows=50):
    sample = df.head(max_rows)
    # basic markdown table
    out = "|" + "|".join(sample.columns.astype(str)) + "|\n"
    out += "|" + "|".join(["---"] * len(sample.columns)) + "|\n"
    for _, row in sample.iterrows():
        out += "|" + "|".join([str(v) for v in row.values]) + "|\n"
    return out

def safe_df(df):
    return df if isinstance(df, pd.DataFrame) else pd.DataFrame()

def status_chip(text: str, color: str = "#3DDC97"):
    st.markdown(f'<span class="wow-badge" style="background:{color}">{text}</span>', unsafe_allow_html=True)

def require_agents_yaml() -> dict:
    # Try local agents.yaml; else allow upload
    cfg = st.session_state.agents_config
    if cfg:
        return cfg
    default_path = "agents.yaml"
    if os.path.exists(default_path):
        try:
            with open(default_path, "r", encoding="utf-8") as f:
                cfg = yaml.safe_load(f) or {}
                st.session_state.agents_config = cfg
                return cfg
        except Exception as e:
            st.warning(f"Failed to read agents.yaml: {e}")
    upl = st.file_uploader("Upload agents.yaml", type=["yaml", "yml"], key="agents_yaml_upl")
    if upl:
        try:
            cfg = yaml.safe_load(upl.read().decode("utf-8")) or {}
            st.session_state.agents_config = cfg
            return cfg
        except Exception as e:
            st.error(f"Invalid YAML: {e}")
    return {}

# ----------------------------
# Header
# ----------------------------
st.markdown("""
<div class="wow-header">
  <span class="wow-badge">Agentic Mind Studio</span>
  <span class="small">Streamlit ‚Ä¢ Gemini ‚Ä¢ OpenAI ‚Ä¢ Grok ‚Ä¢ OCR ‚Ä¢ Mind Graph ‚Ä¢ Dashboard</span>
</div>
""", unsafe_allow_html=True)

# ----------------------------
# Tabs
# ----------------------------
tab_home, tab_file, tab_datasets, tab_graph, tab_agents, tab_logs = st.tabs([
    "Dashboard", "File ‚Üí Markdown + Summary", "Multi-Dataset Analysis", "Mind Graph", "Agents", "Logs"
])

# ----------------------------
# Dashboard Tab
# ----------------------------
with tab_home:
    total_datasets = len(st.session_state.datasets)
    total_records = sum(len(v["df"]) for v in st.session_state.datasets.values() if "df" in v)
    total_nodes = len(set([n for pair in st.session_state.relationships for n in pair])) if st.session_state.relationships else 0
    total_edges = len(st.session_state.relationships)

    st.markdown("### Overview")
    colkpi = st.container()
    with colkpi:
        st.markdown('<div class="kpi-grid">', unsafe_allow_html=True)
        for label, value in [
            ("Datasets", total_datasets),
            ("Records", total_records),
            ("Graph Nodes", total_nodes),
            ("Graph Edges", total_edges),
        ]:
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-value">{value}</div>
                <div class="metric-label">{label}</div>
            </div>
            """, unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)

    if px and total_datasets > 0:
        sizes = []
        names = []
        for ds_id, meta in st.session_state.datasets.items():
            sz = len(meta.get("df", []))
            sizes.append(sz)
            names.append(meta.get("name", ds_id))
        fig = px.bar(x=names, y=sizes, title="Rows per Dataset", labels={"x": "Dataset", "y": "Rows"})
        fig.update_layout(template="plotly_dark", height=300)
        st.plotly_chart(fig, use_container_width=True)

    st.markdown("### Status Indicators")
    cols = st.columns(4)
    with cols[0]: status_chip("Ready", "#3DDC97")
    with cols[1]: status_chip("OCR " + ("OK" if pdfplumber or (pdf2image and pytesseract) else "Unavailable"), "#FFB020")
    with cols[2]: status_chip(f"Provider: {provider}", "#3388FF")
    with cols[3]: status_chip("Theme: " + st.session_state.graph_theme, "#AA66FF")

    st.markdown("### Quick Actions")
    c1, c2, c3 = st.columns(3)
    with c1:
        if st.button("New Graph from Datasets"):
            # auto infer trivial relationships
            recs = []
            for ds in st.session_state.datasets.values():
                recs.extend(ds.get("df", pd.DataFrame()).to_dict(orient="records"))
            st.session_state.relationships = heuristic_relationships(recs)
            st.success("Graph initialized from current datasets.")
    with c2:
        st.write("Upload files in tabs to get started.")
    with c3:
        st.write("Edit agents in Agents tab.")

# ----------------------------
# File ‚Üí Markdown + Summary Tab
# ----------------------------
with tab_file:
    st.markdown("#### Upload or Paste a File (txt, md, pdf) with optional OCR")
    colA, colB = st.columns([2, 1])
    with colA:
        file = st.file_uploader("Upload TXT/MD/PDF", type=["txt", "md", "markdown", "pdf"], key="single_file_upl")
        pasted = st.text_area("Or paste text here", height=180, key="single_paste_text")
    with colB:
        use_ocr = st.toggle("Use OCR for PDF", value=False)
        pages_spec = st.text_input("OCR Pages (e.g., 1-3,5)", value="")
        manual_keywords = st.text_input("Highlight keywords (comma-separated)", value="")
        coral_color = st.color_picker("Keyword color", ACCENT)

    extracted_text = ""
    if file is not None and file.name.lower().endswith(".pdf"):
        pages = parse_pdf_pages(pages_spec) if pages_spec.strip() else []
        with st.status("Extracting from PDF...", expanded=False) as status:
            extracted_text = extract_pdf_text(file, use_ocr, pages)
            if extracted_text.startswith("ERROR"):
                status.update(label="PDF extraction error", state="error")
            else:
                status.update(label="PDF extraction done", state="complete")
    elif file is not None:
        content = file.read().decode("utf-8", errors="ignore")
        extracted_text = content
    elif pasted.strip():
        extracted_text = pasted

    if extracted_text:
        st.markdown("##### Raw Text (editable)")
        edited_text = st.text_area("Edit extracted text", value=extracted_text, height=220, key="edited_text_area")
        kws = [k.strip() for k in manual_keywords.split(",")] if manual_keywords else naive_keywords(edited_text, topn=20)
        highlighted_md = coral_highlight_markdown(edited_text, kws, coral_color)
        st.markdown("##### Markdown with Coral Highlights", help="User can modify keywords or the text above, then regenerate.")
        st.markdown(highlighted_md, unsafe_allow_html=True)

        if st.button("Generate Summary + Entities + Q&A", type="primary"):
            with st.status("Summarizing with LLM (or heuristics)...", expanded=False) as status:
                # provider & model selection for this action
                if provider == "Gemini":
                    mdl = gemini_model
                elif provider == "OpenAI":
                    mdl = openai_model
                else:
                    mdl = grok_model
                data = call_llm_structured(provider, mdl, "summarize", edited_text)
                status.update(label="Summary generated", state="complete")
            # Editable sections
            st.markdown("##### Summary (Markdown)")
            summary_md = st.text_area("Edit summary", value=data.get("summary_markdown", ""), height=260, key="summary_md_editor")
            st.markdown(summary_md)

            st.markdown("##### Entities (100)")
            ents_df = pd.DataFrame(data.get("entities", []))
            ents_edit = st.data_editor(ents_df, use_container_width=True, num_rows="dynamic", key="ents_editor")
            st.markdown("##### Q&A (30)")
            qa_df = pd.DataFrame(data.get("qa", []))
            qa_edit = st.data_editor(qa_df, use_container_width=True, num_rows="dynamic", key="qa_editor")
            st.markdown("##### Follow-up Questions (10)")
            fu_list = data.get("followups", [])
            fu_text = st.text_area("Edit follow-ups (one per line)", value="\n".join(fu_list), height=140)

            st.session_state.last_file_summary = {
                "summary_markdown": summary_md,
                "entities": ents_edit.to_dict(orient="records"),
                "qa": qa_edit.to_dict(orient="records"),
                "followups": [x.strip() for x in fu_text.splitlines() if x.strip()],
                "source_text": edited_text
            }
            st.success("Summary artifacts ready. You can use Agents tab to run an agent on this summary.")

# ----------------------------
# Multi-Dataset Analysis Tab
# ----------------------------
with tab_datasets:
    st.markdown("#### Upload or Paste Multiple Datasets (txt, csv, json)")
    files = st.file_uploader("Upload multiple files", type=["txt", "csv", "json"], accept_multiple_files=True, key="multi_up")
    pasted_multi = st.text_area("Paste dataset content (optional)", height=120, key="multi_paste")

    add_btn = st.button("Add datasets")
    if add_btn:
        added = 0
        if files:
            for f in files:
                df, recs = read_any_dataset(f, None)
                if df is not None:
                    ds_id = str(uuid.uuid4())[:8]
                    st.session_state.datasets[ds_id] = {"name": f.name, "df": df, "json": df.to_dict(orient="records")}
                    added += 1
        if pasted_multi.strip():
            df, recs = read_any_dataset(None, pasted_multi)
            if df is not None:
                ds_id = str(uuid.uuid4())[:8]
                st.session_state.datasets[ds_id] = {"name": f"Pasted-{ds_id}", "df": df, "json": df.to_dict(orient="records")}
                added += 1
        if added > 0:
            st.success(f"Added {added} dataset(s).")
            log_event("datasets:add", f"Added {added} datasets.")

    if st.session_state.datasets:
        st.markdown("##### Datasets")
        for ds_id, meta in list(st.session_state.datasets.items()):
            with st.expander(f"{meta.get('name', ds_id)} (id={ds_id})", expanded=False):
                df = safe_df(meta.get("df"))
                st.dataframe(df.head(100), use_container_width=True)
                st.markdown("Markdown Preview")
                st.code(df_to_markdown(df), language="markdown")
                st.markdown("Edit Table")
                edited = st.data_editor(df, num_rows="dynamic", use_container_width=True, key=f"edit_{ds_id}")
                st.session_state.datasets[ds_id]["df"] = edited
                st.markdown("JSON Preview (editable)")
                json_text = st.text_area("JSON", value=json.dumps(edited.to_dict(orient="records"), ensure_ascii=False, indent=2), height=160, key=f"json_{ds_id}")
                try:
                    st.session_state.datasets[ds_id]["json"] = json.loads(json_text)
                except Exception as e:
                    st.warning(f"JSON invalid for {ds_id}: {e}")

        if st.button("Analyze Across Datasets", type="primary"):
            with st.status("Analyzing across datasets...", expanded=False) as status:
                # Build a compact context
                context_chunks = []
                for ds_id, meta in st.session_state.datasets.items():
                    name = meta.get("name", ds_id)
                    df = safe_df(meta.get("df"))
                    sample_json = json.dumps(df.head(50).to_dict(orient="records"), ensure_ascii=False)
                    context_chunks.append(f"DATASET {name} (id={ds_id}):\n{sample_json}")
                context = "\n\n".join(context_chunks)
                if provider == "Gemini":
                    mdl = gemini_model
                elif provider == "OpenAI":
                    mdl = openai_model
                else:
                    mdl = grok_model
                analyze_prompt = f"""
You analyze multiple datasets together. From the combined input, produce JSON with:
- "report_markdown": comprehensive analysis in markdown, include trends, correlations, anomalies.
- "viz_suggestions": array of 5 strings suggesting further visualizations.
- "entities": 100 entities as objects with fields ["entity","type","salience"].
- "qa": 30 Q&A pairs objects with ["question","answer"].
- "followups": 10 follow-up questions.

Return ONLY valid JSON.
INPUT:
{context}
"""
                out = call_llm_text(provider, mdl, analyze_prompt, temperature=0.2)
                try:
                    if out.strip().startswith("```"):
                        out = "\n".join(out.strip().splitlines()[1:-1])
                    data = json.loads(out)
                except Exception:
                    data = {
                        "report_markdown": "## Heuristic Cross-Dataset Report\n\n- Basic trends by row counts.\n",
                        "viz_suggestions": ["Bar chart of rows per dataset", "Scatter of two numeric columns", "Heatmap correlations", "Sunburst by categorical", "Timeline if time column exists"],
                        "entities": heuristic_entities(context, n=100),
                        "qa": [{"question": "Heuristic Q", "answer": "Heuristic A"} for _ in range(30)],
                        "followups": [f"Heuristic follow-up {i+1}" for i in range(10)]
                    }
                status.update(label="Analysis complete", state="complete")

            st.markdown("##### Cross-Dataset Report (Markdown)")
            report_md = st.text_area("Edit report", value=data.get("report_markdown", ""), height=280, key="cross_report_md")
            st.markdown(report_md, unsafe_allow_html=False)

            st.markdown("##### Visualization Suggestions (5)")
            vs = data.get("viz_suggestions", [])
            vs_txt = st.text_area("Edit viz suggestions", value="\n".join(vs), height=120, key="viz_suggestions")
            st.markdown("##### Entities (100)")
            ents_df = pd.DataFrame(data.get("entities", []))
            ents_edit = st.data_editor(ents_df, use_container_width=True, num_rows="dynamic", key="cross_entities")
            st.markdown("##### Q&A (30)")
            qa_df = pd.DataFrame(data.get("qa", []))
            qa_edit = st.data_editor(qa_df, use_container_width=True, num_rows="dynamic", key="cross_qa")
            st.markdown("##### Follow-up Questions (10)")
            fu_txt = st.text_area("Edit follow-ups", value="\n".join(data.get("followups", [])), height=140, key="cross_followups")

            st.session_state.cross_analysis = {
                "report_markdown": report_md,
                "viz_suggestions": [x.strip() for x in vs_txt.splitlines() if x.strip()],
                "entities": ents_edit.to_dict(orient="records"),
                "qa": qa_edit.to_dict(orient="records"),
                "followups": [x.strip() for x in fu_txt.splitlines() if x.strip()],
            }
            st.success("Cross-dataset analysis artifacts ready.")

# ----------------------------
# Mind Graph Tab
# ----------------------------
with tab_graph:
    st.markdown("#### Build and Explore a Mind Graph")
    st.markdown("Upload or paste a dataset to infer relationships. You can also filter and search nodes.")
    colX, colY = st.columns([2, 1])
    with colX:
        uploaded = st.file_uploader("Upload CSV / JSON / TXT for graph", type=["csv","json","txt"], key="graph_upl")
        pasted_text = st.text_area("Or paste content", height=120, key="graph_paste")
        custom_instruction = st.text_area("Optional guidance to LLM", height=80, key="graph_guidance")
    with colY:
        infer_button = st.button("Infer Relationships", type="primary")
        filter_kw = st.text_input("Filter nodes by keyword", value="")
        search_node = st.text_input("Search node label", value="")

    df, records = read_any_dataset(uploaded, pasted_text)
    if df is not None:
        st.dataframe(df.head(), use_container_width=True)

        if not st.session_state.relationships:
            st.session_state.relationships = heuristic_relationships(records)

        def prompt_relationships(records, instruction=None):
            ctx = "\n".join([json.dumps(r, ensure_ascii=False) for r in records[:50]])
            base = "Extract main topics, sub-topics, and relationships. Return ONLY a valid Python list of tuples [('Source','Target'), ...]."
            if instruction:
                base += f"\nAdditional guidance: {instruction}"
            if provider == "Gemini" and google_genai:
                try:
                    model = google_genai.GenerativeModel(gemini_model)
                    resp = model.generate_content(f"Records:\n{ctx}\n\nTask:\n{base}")
                    text = getattr(resp, "text", "").strip()
                    if text.startswith("```"): text = "\n".join(text.splitlines()[1:-1])
                    return ast.literal_eval(text)
                except Exception as e:
                    st.warning(f"Gemini error: {e}")
                    return heuristic_relationships(records)
            elif provider == "Grok" and xai_Client:
                try:
                    client = xai_Client(api_key=grok_key, timeout=3600)
                    chat = client.chat.create(model=grok_model)
                    chat.append(xai_grok_user(f"Records:\n{ctx}\n\nTask:\n{base}"))
                    resp = chat.sample()
                    text = resp.content.strip()
                    if text.startswith("```"): text = "\n".join(text.splitlines()[1:-1])
                    return ast.literal_eval(text)
                except Exception as e:
                    st.warning(f"Grok error: {e}")
                    return heuristic_relationships(records)
            elif provider == "OpenAI" and openai:
                try:
                    prompt = f"Records:\n{ctx}\n\nTask:\n{base}"
                    kw = {"model": openai_model, "messages": [{"role": "user", "content": prompt}]}
                    if hasattr(openai, "chat") and hasattr(openai.chat, "completions"):
                        r = openai.chat.completions.create(**kw)
                        text = r.choices[0].message["content"].strip()
                    else:
                        r = openai.ChatCompletion.create(**kw)
                        text = r["choices"][0]["message"]["content"].strip()
                    if text.startswith("```"): text = "\n".join(text.splitlines()[1:-1])
                    return ast.literal_eval(text)
                except Exception as e:
                    st.warning(f"OpenAI error: {e}")
                    return heuristic_relationships(records)
            else:
                st.warning("No API key provided or provider unavailable. Using heuristic extraction.")
                return heuristic_relationships(records)

        if infer_button:
            with st.status("Inferring relationships...", expanded=False) as s:
                st.session_state.relationships = prompt_relationships(records, custom_instruction)
                s.update(label="Relationships inferred", state="complete")

        rels_df = pd.DataFrame(st.session_state.relationships, columns=["source","target"])
        edited_df = st.data_editor(rels_df, num_rows="dynamic", use_container_width=True, key="graph_editor")
        st.session_state.relationships = list(edited_df.itertuples(index=False, name=None))

        # Filter nodes
        relationships = st.session_state.relationships
        if filter_kw.strip():
            relationships = [pair for pair in relationships if filter_kw.lower() in pair[0].lower() or filter_kw.lower() in pair[1].lower()]

        nodes, edges, node_index = build_graph_elements(relationships, node_color, graph_theme)
        config = Config(
            width=1100,
            height=600,
            directed=True,
            physics=True,
            nodeHighlightBehavior=True,
            highlightColor=graph_theme["edge"],
            bgcolor=graph_theme["bg"],
            font={"color": graph_theme["font"], "size": 12}
        )
        st.subheader("Interactive Mind Graph")
        selected = agraph(nodes=nodes, edges=edges, config=config)

        # Node search
        if search_node.strip():
            if any(n.label == search_node for n in nodes):
                st.info(f"Found node: {search_node}")
            else:
                st.warning("Node not found.")

        # Node click details
        if df is not None and records:
            if selected:
                if isinstance(selected, dict) and "nodes" in selected and selected["nodes"]:
                    node_id = selected["nodes"][0]
                    label = [n.label for n in nodes if n.id == node_id][0]
                    st.write(f"Node: {label}")
                    st.json(records_by_node(label, records))
                elif isinstance(selected, list) and len(selected) > 0:
                    node_id = selected[0]
                    label = [n.label for n in nodes if n.id == node_id][0]
                    st.write(f"Node: {label}")
                    st.json(records_by_node(label, records))
                else:
                    st.info("Click a node to view related records.")
            else:
                st.info("Click a node to view related records.")
    else:
        st.info("Upload or paste a dataset for graphing.")

# ----------------------------
# Agents Tab
# ----------------------------
with tab_agents:
    st.markdown("#### Choose an Agent from agents.yaml and Run")
    cfg = require_agents_yaml()
    if not cfg:
        st.info("Provide agents.yaml to continue.")
    else:
        agents = cfg.get("agents", [])
        names = [a.get("name", f"agent_{i}") for i, a in enumerate(agents)]
        if not names:
            st.warning("No agents found in agents.yaml (expect top-level 'agents' array).")
        else:
            agent_name = st.selectbox("Select agent", names)
            agent = next((a for a in agents if a.get("name") == agent_name), None) or {}

            # Editable agent yaml
            raw_yaml = st.text_area("Edit agent YAML", value=yaml.safe_dump(agent, allow_unicode=True, sort_keys=False), height=220)
            try:
                agent = yaml.safe_load(raw_yaml) or {}
            except Exception as e:
                st.error(f"Agent YAML invalid: {e}")

            # Pick model per agent
            agent_provider = st.selectbox("Provider for this agent", ["Gemini", "OpenAI", "Grok"], index=["Gemini", "OpenAI", "Grok"].index(provider))
            if agent_provider == "Gemini":
                agent_model = st.selectbox("Model", ["gemini-2.5-flash", "gemini-2.5-flash-lite"])
            elif agent_provider == "OpenAI":
                agent_model = st.selectbox("Model", ["gpt-5-nano", "gpt-4.1-nano"])
            else:
                agent_model = st.selectbox("Model", ["grok-3-mini", "grok-4-fast-reasoning"])

            system_prompt = agent.get("system_prompt", "You are a helpful agent.")
            user_prompt = st.text_area("User prompt", value=agent.get("user_prompt", ""), height=160)

            # Inputs source
            st.markdown("Input Source")
            src_opt = st.radio("Choose input", ["Manual", "Last File Summary", "Cross-Dataset Analysis"])
            if src_opt == "Manual":
                input_text = st.text_area("Manual input", height=180, key="agent_manual_input")
            elif src_opt == "Last File Summary":
                input_text = (st.session_state.get("last_file_summary", {}) or {}).get("summary_markdown", "")
                st.text_area("Preview", value=input_text, height=160)
            else:
                input_text = (st.session_state.get("cross_analysis", {}) or {}).get("report_markdown", "")
                st.text_area("Preview", value=input_text, height=160)

            temp = float(st.slider("Temperature", 0.0, 1.0, float(agent.get("temperature", 0.2)), 0.05))
            run = st.button("Run Agent", type="primary")
            if run:
                with st.status("Executing agent...", expanded=False) as status:
                    full_prompt = f"System: {system_prompt}\n\nUser: {user_prompt}\n\nContext:\n{input_text}"
                    resp = call_llm_text(agent_provider, agent_model, full_prompt, temperature=temp)
                    status.update(label="Agent run complete", state="complete")
                st.markdown("##### Agent Response (editable)")
                resp_edit = st.text_area("Edit response", value=resp, height=260, key="agent_resp_edit")
                st.success("You can refine and copy the response.")

# ----------------------------
# Logs Tab
# ----------------------------
with tab_logs:
    st.markdown("#### Activity Log")
    if st.session_state.logs:
        st.json(st.session_state.logs)
    else:
        st.info("No logs yet.")

```

What‚Äôs new highlights:
- Wow UI: glassmorphism cards, gradient app background, badges, KPI grid, and refined tabs; themeable graph palette and node color.
- Status indicators: st.status used for OCR, summarization, relationship inference, and analysis; badge chips show current system state; quick actions on dashboard.
- Interactive dashboard: KPIs, bar chart of dataset sizes (Plotly), provider/theme status.
- File-to-Markdown pipeline: PDF OCR by selected pages; coral keyword highlighting; editable summary, entities (100), Q&A (30), follow-ups (10).
- Multi-dataset pipeline: multi-upload/paste; per-dataset editable tables and JSON; cross-dataset comprehensive analysis with visualization suggestions and the same 100/30/10 artifacts.
- Agents: load/edit from agents.yaml, choose provider/model per agent, edit prompts/params, run agent, and edit output.
- Mind Graph: relationship inference via chosen provider/model; filtering/search; enhanced theming.

Notes:
- OCR: If pdf2image/pytesseract are not available, the app shows a clear message. On HF Spaces, install system package tesseract-ocr and Python packages pdf2image and pytesseract. For non-OCR extraction, pdfplumber is used if available.
- OpenAI client versions vary; the code tries both new and legacy method signatures.
- agents.yaml format expected: top-level key "agents" with a list of agent objects including name, system_prompt, user_prompt, temperature, etc.

Would you like me to:
- Provide a minimal agents.yaml template?
- Add optional PyVis graph and more dashboard charts (correlation heatmaps, category treemaps)?
- Persist edited artifacts to local files for download on Hugging Face Spaces?
